\section{Datasets \& Methodology}\label{sec:methodology}

In this section we outline our measurement technique for establishing a breadth
based understanding of bidirectional censors on the internet. We cover the
details of the various probes that we send as well as the sources from which we
draw target addresses and domains that trigger censorship responses.

\subsection{Selecting target domains}
\label{sec:methodology:domains}
Bidirectional censorship is triggered whenever a censor detects a domain present 
in its block-list of domains to censor. The Citizen Lab \cite{TheCitiz6:online}
maintains a list of domains \cite{testlist} which are known to be present 
in censor's block-lists. These lists contain country-specific domains i.e\ domains that are tested specifically for each country with country specific context e.g\ local language and global domains which contain content e.g\ freedom of expression, gambling e.t.c.\ known to be considered sensitive in most censoring countries. For this study, we used the global list to conduct our measurements as we are concerned with measuring the prevalence of bidirectional censorship globally i.e\ how many countries deploy bidirectional censorship. The global list contains $\sim$1400 domains.

We supplement this set with 10 control domains. These controls are not present in any
block-list, nor are they are not sub-domains or substring matches of any block-listed domain in a block-list that we are aware of. 3 of these domains are hosted in the US and provide valid A and AAAA record.
% As our primary goal is to identify the prevalence of bidirectional censorship
% capabilities at local, organizational, or national scale we require a list of
% domains that will trigger censorship responses. We begin by selecting domains
% from censored planet lists, then for specific case studies we use more targeted
% lists of domains known to be censored in specific - i.e. the Roskomnadzor
% censorship domain name list for Russia~\cite{}. If no well coordinated public
% list is available, for country specific case studies we use the censored planet
% list tailored to the individual country.

% We supplement this set with multiple control domains that we host in the US that
% provide valid A and AAAA records. These controls are not present in any
% blocklist, nor are they are not subdomains or substring matches of any
% blocklisted domain in a blocklist that we are aware of. This provides a baseline
% against which we can determine whether or not responses are ``normal'' when
% requests contain blocklisted domain names.

\subsection{Selecting IP Addresses}
\label{subsec:selecting-ips}
Our goal in selecting target IP addresses for each experiment is to identify a set of
non-responsive hosts within IP address allocations that transit a link monitored
by a censor such that benign control probes receive a predictable response or no
response at all, but sensitive domains trigger characteristic responses from the
on path censor.

As we wanted a truly global view of bidirectional censorship, we wanted to test as many countries and as many allocations in each country as possible. To this end, we collected a list of all IP allocations announced by all 5 regional registries \cite{herrbisc56:online}. This list contained $\sim$155,000 IPv4 and $\sim$63,000 IPv6 allocations from a total of $\sim$200 countries (and regions). We then used MaxMind \cite{IPGeoloc87:online} to assign the Autonomous System numbers and organization names to each of these allocations. Our final allocations consisted of all allocations from all organizations that had at least 1 IPv4 and 1 IPv6 allocation resulting in 
$\sim$71,000 IPv4 and $\sim$20,000 IPv6 allocations spanning all the countries in our initial dataset.   

These allocations contain the whole IP block that the regional registries assigned to regional organizations. However, part of this IP block might remain unused and hence unannounced by the organization. Choosing IP addresses from the unannounced region of an IP block might never be routed to the country the IP block belongs to and a censorship response would not be triggered by the country's on path censor. This would result in under-representation of the true prevalence of bidirectional censorship in a country. So instead of using IP addresses in allocated prefixes for testing bidirectional censorship, we chose IP addresses only from the announced prefixes of each allocation. We used the \textit{University of Oregon Route Views Project} \cite{RouteVie20:online}
to get all the announced prefixes for each of the allocated IP block. This resulted in a total of 
$\sim$313,000 IPv4 and $\sim$40,000 IPv6 announced IP prefixes. 

For each of the announced prefix, both IPv4 and IPv6, we select a set of $N=10$
addresses at random. We arrived at this number after a couple of considerations. First, scanning the entirety of the addresses space for $\sim$1400 domains is infeasible for IPv4 and impossible for IPv6. Second, our aim is to test for on path censorship on the penultimate hop and not reach the end hosts themselves. This technique allows us to test for bidirectional censorship while maintaining the breadth of our measurements. Choosing 10 addresses from each of the announced prefix in our allocation dataset amounted to $\sim$3.5 million IP addresses.
% Fortunately, a significant majority of ipv4 addresses (and an even more
% overwhelming majority in IPv6) are not servers that provide access to the
% services that we measure. In fact, around 1\% of addresses in the IPv4 space
% will complete a TCP handshake for port 443~\cite{}. The rate is similar for
% TCP:80 and UDP:53. UDP:443 is even more rare as Quic is a developing
% protocol and DTLS is not a commonly used. As such addresses chosen at random are
% overwhelmingly likely to not host servers that respond to benign requests.
% % For each individual protocol that we measure we need to target non-responsive
% addresses, or understand benign ''go away'' responses elicited by control
% probes. One way that we can do this is by using the
% Zmap~\cite{Durumeric13zmap} tool to perform a port scan on the port associated
% with the protocol (e.g. TCP443 for TLS, UDP53 for DNS, etc.) and filter out
% addresses that are at all responsive. For example, we scan TCP 443 across the
% whole IPv4 space using a syn probe to identify the list of responsive addresses.
% When selecting addresses we then incorporate this list by actively avoiding
% responsive hosts.

% Given the significant number of domains that we endeavor to include in our
% measurement it is impractical to scan the entire IPv4 space for each domain.
% Further, given the scale of the IPv6 addresses space it is impossible to scan
% all available addresses for even one domain. In order to achieve a
% representative result in light of these limitations we select a set of $N=5$
% addresses at random from each announced IPv4 and IPv6 subnet allocations. While
% addresses are typically not used at random within subnet allocation and our
% address selections do not represent this human factor, our goal is to test for
% on path censorship up to the penultimate hop, not reach the end hosts
% themselves. We believe that this breadth based strategy overcomes limitations of
% which portions of allocated subnets are unrouted or privately subdivided as the
% probes route {\bf towards} the allocated subnet testing a majority of the paths
% on the internet with tunable redundancy for censorship responses along the way.

\subsection{Identifying Bidirectional Censorship}
\label{sec:methodology:censorship}

We focus on identifying bidirectional censorship via injected responses to
ingress traffic. This allows us to send probes from external vantage points and
receive responses that we can classify as expected or characteristic of
censorship. We do this for several different protocols that are known (or
suspected) to be censored.


% dropped connections (we cannot find) - but we are not trying to measure inline blocking

\FigProbeSend

\subsubsection{TCP}
While some firewall implementations explicitly look for singular TCP (TLS, HTTP,
or other) packets that violate their rules, others keep a modest amount of state
and require the TCP flow to be ``established'' before they will present
censorship behavior. However, because routing on the internet is not typically
symmetrical and response traffic often follows alternative routes, some high
performance firewall implementations will trigger censorship behavior with just
the unidirectional client-to-station flow of a TCP SYN packet, followed by an
ACK packet and a data packet with the PSH/ACK flags set. While this allows flows
that use heterogeneous routing to be included and censored, it also allows
falsified flows with non-existent endpoints to trigger a censorship response as
there is no validation that the TCP handshake successfully completed.

We take advantage of this by measuring censorship responses triggered by a
singular TCP PSH/ACK packet with data, as well as responses triggered by a
packet sequences of SYN -- ACK -- PSH/ACK with data.

Our \textbf{HTTP} probe data consists of a HTTP request crafted to trigger
censorship using blocklisted domains in the \texttt{HOST} header.
% Add/ Test Keyword based blocking?

Our \textbf{TLS} probe data consists of a TLS \texttt{ClientHello} crafted to
trigger censorship using blocklisted domains in the SNI extension.

\subsubsection{UDP}
To measure \textbf{DNS} we send one {\tt A} and one {\tt AAAA} query per domain
to each of our selected target addresses. This allows us to make a loose, but
direct comparison of censorship rates for IPv4 and IPv6 resource records.


Our \textbf{Quic} probe consists of a client initial frame that contains a
TLS1.3 {\tt ClientHello} with the domain in question in the SNI extension. In
following with the Quic specification the payload portion of the frame is
encrypted using an a key derived using an HKDF on our selected connection ID.
While on-path attackers can decrypt this initial packet and inspect the
plaintext TLS packet including SNI, they are forced to perform a relatively
costly HKDF in order to do so.

\subsubsection{Tagging}

Similar to the architecture of the zmap scanning tool, our probing architecture
uses many threads to craft and send packets and one independent thread to listen
and ingests responses. One consequence of this architecture is that we must
maintain a limited amount of state internally for each connection we create.
While injected responses to DNS probes may include the host name (in the
response Resource Record) other protocols are not guaranteed to do so. For
example, the TCP RST packets injected by the GFW in response to a TLS probe with
a censored SNI will not indicate what domain the original probe included.
Similar challenges exists for HTTP and Quic probes.

To solve this problem we employ a tagging system for outgoing packets such that
we can identify the details of the probe they correlate to and check the
validity of the response without tracking the full connection state from start
to finish.

We start by creating a 1-to-1 mapping from domain to a random number in the
range 1000-65535. We use this number as the source port for the outgoing packet
meaning that we can use the destination port of any response packet to lookup
the domain sent in the original probe. In order to ensure that response TCP
packets are associated with our measurement and not just sent randomly we set
the acknowledgement number of the outgoing probe to be the CRC32 of the source
port (from our domain mapping) and the target address. This allows our ingest
thread to quickly validate responses by checking:
\begin{gather*}
CRC32(PORT_{dst},ADDR_{src}) \stackrel{?}{=} SEQ - Len
\end{gather*}

For Quic responses that either return garbage or change the connection ID for
the server initial packet we need to have access to the 8 byte connection ID
sent in our probe. To make sure this is always available we set the source port
for the outgoing packets from our 1-to-1 domain map and then set the connection
ID to be the CRC64-ECMA of the source port and the target address for the
outgoing packet. That way response packets can statelessly derive the original
connection ID by computing the following for incoming packets.
\begin{gather*}
Conn\_ID = CRC64_{ECMA}(PORT_{dst},ADDR_{src})
\end{gather*}


\subsection{Ethics}\label{sec:methodology:ethics}

Our experimental design has incorporated ethical considerations into the
decision-making process at multiple stages. Censorship measurement has inherent
risks and trade-offs: better understanding of censorship can help support and
inform users, but specific measurements may carry risk to participants or
network users. Measurement of bidirectional censorship typically allows
researchers to limit the number of third parties implicated in experiments as
the censorship response can be triggered by either ingress or egress traffic
removing the need for cooperation by individual hosts or hosting services within
a censoring region. Vantage points are instead hosted in regions that do not
censor connections and allow for researchers to freely measure the internet.

The vantage that was used for data collection is connected to the internet with
a 1 Gbps interface that scanned using the default rates for our custom protocol
scanning tool (line rate). However, the structure of the scan was established
such that individual addresses and domains would be accessed in round robin
order --- \ie when sending probes every target address would receive a first
request before any target would receive the subsequent request.

We encourage readers to consult The Menlo Report~\cite{menlo}, its companion
guide~\cite{menlo-companion}, and the censorship specific ethical measurement
guidelines outlined by Jones \etal \cite{jones2015ethical} for further
discussion of ethical design for internet measurement.