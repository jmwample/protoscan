\section{Dataset and Methodology}\label{sec:methodology}



% The analysis presented in the remainder of this paper is based on the results of
% 22.4M DNS {\tt A} and {\tt AAAA} resolution requests for 714 domains sent to
% 7,843 IPv4- and IPv6-capable resolvers located in 128 countries.
% %
% In this section, we explain our process for identifying resolver targets for
% our queries (\Cref{sec:methodology:resolvers}), domains that are the subject of
% our queries (\Cref{sec:methodology:domains}), and our process for identifying
% the occurrence of a censorship event from the results of each query
% (\Cref{sec:methodology:censorship}).


\subsection{Selecting target domains}
\label{sec:methodology:domains}
As our primary goal is to identify the prevalence of bidirectional censorship
capabilities at local, organizational, or national scale we require a list of
domains that will trigger censorship responses. We begin by selecting domains
from censored planet lists, then for specific case studies we use crafted
targeted lists of domains known to be censored - i.e. the Roskomnadzor
censorship domain name list or Russia.

\subsection{Selecting IP Addresses}
\label{subsec:selecting-ips}
We select IP addresses dynamically at the time of each experiment in order to
accomplish  a specific goal. The goal is to identify non-responsive addresses in
censoring allocations such that benign control probes receive no response. This
provides a contrast that allows us to identify abnormal censorship responses.

IPv4 and IPv6

For each individual protocol that we measure we need to target non-responsive
addresses, or understand benign ''go away'' responses that we expect in response
to control probes. One way that we can do this is by using the
Zmap~\cite{Durumeric13zmap} tool to perform a port scan on the port associated
with the protocol (e.g. TCP443 for TLS, UDP53 for DNS, etc.) and filter out
addresses that are at all responsive. For example, we scan TCP 443 across the
whole IPv4 space using a syn probe to identify the list of responsive addresses.
When selecting addresses we then incorporate this list by actively avoiding
responsive hosts.


Given the significant number of domains that we endeavour to include in our
measurement it is impractical to scan the entire IPv4 space for each domain.
Further, given the scale of the IPv6 addresses space it is impossible to scan
all available addresses with even one domain. In order to achieve a
representative result in light of these limitations we select a set of $N$
addresses at random from each announced IPv4 and IPv6 subnet allocations for
each probe. While addresses are typically not used at random within subnet
allocation and our address selections do not represent this human factor, our
goal it to test the penultimate hops for censorship, not reach the end hosts
themselves. In this sense we believe that this methodology allows us to test a
majority of the paths on the internet for censorship responses to our crafted
probes.

\subsection{Identifying Censorship Events}

We focus on identifying bidirectional censorship via injected responses to
ingress traffic. This allows us to send probes from external vantage points and
receive responses that we can classify as expected or characteristic of
censorship. We do this for several different protocols that are known or
suspected to censored.

Within our set of test domains we include a set of control domains that provide
us with confidence about the validity of our censorship classification per IP.

\subsubsection{DNS} \label{sec:methodology:censorship}

% Injected DNS response record.
Because UDP is not a reliable protocol and there is no universal ``go away''
message any resource record that we receive in response to the probes are
treated as indicative of censorship.

% A vs AAAA
For our measurement we send one {\tt A} and one {\tt AAAA} query per domain to
each of our selected target addresses. This allows us to make a loose, bt direct
comparison of censorship rates for IPv4 and IPv6 resource records.

\subsubsection{HTTP}

injected block Pages

Injected TCP RST

% Keyword?

\subsubsection{TLS}

Injected TCP RST

SYN ACK PSK/ACK

\subsubsection{Quic}

Injected Garbage frame (would be nice to find)

\medskip

dropped connections (we cannot find) - but we are not trying to measure inline blocking

% \subsubsection{DNS} \label{sec:methodology:censorship}
% In general, our goal is to err on the side of caution and avoid false-positives
% in our censorship determination. We achieve this by accounting for
% unreliability of resolvers and domains in our lists.

% \para{Removing unstable resolvers.}
% For each of the 7,843 (IPv4, IPv6) resolver pairs and 714 sensitive domains, we
% send a DNS {\tt A} and {\tt AAAA} resource record request for a domain to the
% IPv4 and IPv6 addresses associated with the resolver. We follow this up with
% a {\tt A} and {\tt AAAA} resource record request for a set of 3 control domains
% (owned and operated by us).
% %
% We discard data from pairs which failed to resolve any one of our control
% domains correctly since this is a sign of resolver instability. This left us
% with 7,441 stable resolver pairs.
% %

% \para{Distinguishing censorship from domain instability.}
% For the remaining resolvers, we extract the IPv4 and IPv6 resource records
% returned for each domain --- even those that arrive from multiple responses to
% a single query (a sign of an on-path censor). Next, we use \texttt{zgrab2} to
% establish TLS connections to the IP addresses contained in the resource
% records. In each of these connections, we set the TLS SNI to be the domain
% whose records were requested. We then locally verify the validity of the
% retrieved TLS certificates.
% %
% Since the domains themselves might be unreliable, we repeat this verification
% procedure three times. Only if this step fails all three times do we conclude
% that the IP we obtained from that resolver for that domain was censored.



\subsection{Ethics}\label{sec:methodology:ethics}
Our experimental design has incorporated ethical considerations into the
decision-making process at multiple stages.
%We concern ourselves primarily with
%the security and autonomy of human users and their ability to securely access
%the internet.
%Given the prevalence of self-censorship, understanding and
%documenting the mechanisms and scope of censorship strategies can assist users
%in better evaluating their own risk. However, censorship measurement naturally
%entails interacting with and occasionally violating the rules of access control
%systems.
Censorship measurement has inherent risks and trade-offs: better understanding
of censorship can help support and inform users, but specific measurements may
carry risk to participants or network users.
We rely on The Menlo Report~\cite{menlo}, its companion
guide~\cite{menlo-companion}, and the censorship specific ethical measurement
guidelines discussed by Jones \etal \cite{jones2015ethical} to
carefully weigh these trade-offs in our experimental design.
%provide structure and guidance to our experimental design.

\para{Consent.}
To align with the guiding principle of \textit{respect for persons} we
structure the data collection to implicate as few individuals as possible.
Specifically we rely on open resolvers which typically have little or no direct
association with individuals in lieu of measurement from client based
software. While we cannot acquire direct or proxy consent from the operators of
the open resolvers we consider the trade-off between the implied consent
standard and the value in the measurements we make. 
%Understanding the extent
%and mechanisms of censorship infrastructure can help to demystify and clarify
%risks to real users seeking to safely access the internet. It is important to
We note that the goal of our ethical analysis is not to eliminate risk, but to
minimize it wherever possible. As noted by Jones \etal in some cases acquiring
consent from operators may not only be impossible, but could increase the risk to
operators as it introduces their acknowledgement of, or active participation
in, the measurement at hand~\cite{jones2015ethical}. This analysis aligns with
previous work relying on open resolvers to collect impactful results while
minimizing risk on
individuals~\cite{pearce2017global,scott2016satellite,sundara2020censored}.

\para{Privacy.}
Our study collects no personal data about any end~users, or end hosts, or
network operators. The analysis completed herein uses randomly selected
addresses, public Anonymous System (AS) identifiers, and country codes.  All
measurements are initiated from within the United States.
%at little to no risk of
%repercussions to citizens in the countries that we examine.
Beyond this,
measurement domains are not drawn from any human browsing patterns or history
as the suspected censored domains are a subset of the Satellite measurement
results (\cf \Cref{sec:methodology:domains}).

\para{Resource usage.}
The vantage that was used for data collection is connected to the internet with
a 1 Gbps interface that scanned using the default rates for {\tt zmap} and our
custom protocol scanning tool (line rate). However, the structure of the scan
was established such that individual addresses and domains would be accessed in
round robin order --- \ie when sending probes every target address would
receive a first request before any target would receive the subsequent request.
% Equivalently for validating TLS certificates, each of the 714 target domains
% would receive a first attempted handshake before any target would receive a
% subsequent handshake, limiting the bandwidth any one host will receive.

%While we do not have an upper bound on the bandwidth that was sent
%to individual resolvers or TLS endpoints we believe this strategy provides
%a reasonable limitation to the impact of the measurements in this study.

