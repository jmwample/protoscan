\section{Datasets \& Methodology}\label{sec:methodology}

In this section we outline our technique for measuring bidirectional censorship
globally. While this technique can miss several kinds of censorship (e.g.
censorship limited to a country, or IP-based blocks), it allows us to
efficiently measure censorship around the world from a single vantage point.
%We emphasize there may be other techniques...
Bidirectional censorship occurs when a country's firewall is agnostic to the
direction that packets cross the border, and injects responses even if the
offending request or connection originates outside the country. This allows us
to send a censored packet into the country from our vantage point in
North America, and receive back injected responses.

 %establishing a breadth
%ased understanding of bidirectional censors on the internet. We cover the
%etails of the various probes that we send as well as the sources from which we
% raw target addresses and domains that trigger censorship responses.

\FigProbeSend

\subsection{Selecting target domains}
\label{sec:methodology:domains}
We begin by selecting a list of domains that are likely to trigger injected
responses from censoring countries around the world.
%Bidirectional censorship is triggered whenever a censor detects a domain present
%in its block-list of domains to censor. 
We use The Citizen Lab's~\cite{TheCitiz6:online} domain test
list~\cite{testlist}, which includes domains that are known to be blocked by
many censors. We use the global test list (composed of 1397 unique domains) for
our measurements, rather than country-specific lists, to keep our experiments
consistent across countries.
%\todo{How many exactly? What date was it downloaded?}

%maintains a list of domains \cite{testlist} which are known to be present in the
%block-lists of many censors. These lists contain country-specific domains
%i.e\ domains that are tested specifically for each country with country specific
%context (e.g\ local language) and global domains which contain content (e.g\
%freedom of expression, gambling e.t.c.) commonly considered sensitive in
%censoring countries. For this study, we used the global list to conduct our
%measurements as we are concerned with measuring the prevalence of bidirectional
%censorship globally i.e\ which countries deploy bidirectional censorship
%strategies. The global list contains approximately 1400 domains. \todo{Exact
%number of domains?}

We supplement this list with 10 control domains that to our knowledge are not
blocked by any censors. Our control domains are a combination of domains we set up
specifically for these measurements, and domains that have not been registered
(produce an NXDOMAIN).
If an IP responds to requests (e.g. DNS, HTTP,
TLS, etc) containing our control domains, we assume that IP is a legitimate host
or non-censoring firewall (e.g. paywall or corporate firewall), and remove the
IP from our study. Thus, we use control domains to locate \emph{unused} IPs that
don't respond to our control queries, and test for in-network injections to
these IPs.

%not present
%in any block-list, nor are they are sub-domains or substring matches of any
%domain in a block-list that we are aware of. Three of these domains are
%hosted in the US and provide valid A and AAAA records. This provides a baseline
%against which we can determine whether or not responses are ``normal'' when
%requests contain blocklisted domain names.

\subsection{Selecting IP Addresses}
\label{subsec:selecting-ips}

\FigAllocSize

Our goal is to identify destinations that we can send probes to that will route
past a particular country's censorship infrastructure, but not reach a
responsive host. Ideally, our probe either triggers censorship (if it is on path) and
receives an injection, or the probe is dropped by a router or host in the censoring
country. To achieve this, we select IP addresses to scan that are routed, in a
given country, but ultimately
non-responsive to our control domain probes.

%
%Our goal in selecting target IP addresses for each experiment is to identify a
%set of non-responsive hosts within IP address allocations that transit a link
%monitored by a censor such that benign control probes receive a predictable
%response or no response at all, but sensitive domains trigger characteristic
%responses from the on path censor.

% In order to achieve a representative global view of bidirectional censorship
% we test as many countries and as many allocations in each country as possible.
We begin by collecting a list of all IP allocations announced by the 5 regional
registries~\cite{herrbisc56:online} containing 155k IPv4 and
63k IPv6 allocations from a total of over 200 countries. We use
MaxMind~\cite{IPGeoloc87:online} to assign the Autonomous System Numbers (ASNs)
and organization names to each of these allocations. We then filter out
organizations that do not have at least one IPv4 and one IPv6 allocation
resulting in 71k IPv4 and 20k IPv6 allocations.

It is uncommon that the entire IP allocation provided by RIRs is announced as
is, often being split and announced in several smaller allocations. Portions of
the allocated IP block sometimes remain unannounced by the organization.
Choosing IP addresses from the unannounced region of an IP block might reduce
the rate at which probes are truly routed into the country in question
potentially resulting in a false negative, under-representing the prevalence of
bidirectional censorship in a country. To increase the confidence that all
chosen IP addresses are routable and lower the chance of this type of false
negative result we chose IP addresses only from the announced prefixes of each
allocation. We used the \textit{University of Oregon Route Views Project}
\cite{RouteVie20:online} to get all the announced prefixes for each of the
allocated IP block, as of \todo{What date did we download this?}. This resulted
in a total of 295,385 IPv4 and 39,572 IPv6 prefixes representing 186 countries.


%could simplify or cut - we choose 10 addresses randomly from each prefix,
%resulting in 3.5 million IP addresses total.
For each announced prefix, we select a set of $N=10$ addresses at random. We
arrived at this number after a couple of considerations. First, scanning the
entirety of the addresses space for 1400 domains is infeasible for IPv4 and
impossible for IPv6. Second, our aim is to test for on path censorship on the
penultimate hop and not necessarily reach the end hosts themselves. This
technique allows us to test for bidirectional censorship while maintaining the
breadth of our measurements. Choosing 10 addresses from each of the announced
prefix in our allocation dataset results in over 3.3 million IP addresses.

\subsection{Identifying Bidirectional Censorship}
\label{sec:methodology:censorship}


\textbf{\Cref{fig:probeSend}} shows a high-level overview of measuring
bidirectional censorship from a single vantage point outside the censoring
country. For each domain in our test list, we send a probe to each IP address,
for several different protocols, and observe if we receive an (injected)
response. If we receive a response for non-control domains (and no response for
the control domains), we mark the IP address as likely censored. For instance,
sending a DNS request for \texttt{youtube.com} to an IP address in China usually
results in receiving an injected DNS response from China's Great Firewall, while
sending a query for an uncensored domain we would expect no response.

For each experiment, we send a request for a given domain to each IP in our
3.5~million selected IPs before moving to the next domain. This avoids
overwhelming any individual IP address, as each IP receives a probe
approximately every 9~seconds during our scans.

\subsubsection{Protocols}
We scan for censorship in several protocols, including DNS, HTTP, and TLS.

\paragraph{DNS} For each domain from our list, we craft a DNS query for both A
and AAAA records. We note that each of these queries can be sent to an IPv4 or
IPv6 address, allowing us to observe if the censor can process IPv6 packets or
handles AAAA records properly.

\paragraph{HTTP} Plaintext HTTP is often censored if the Host header contains 
a censored domain. We craft a simple GET request with the domain in the Host
header in order to trigger censorship. % TODO: user agent? Other headers?
Since HTTP is sent over TCP, censors may track connection
state, expecting to see at least the client's side of a TCP handshake before a
request in order for the censor to inject a response~\cite{bock2021your}. For
this reason, we send two kinds of HTTP probes. First, we send a plain HTTP
request in a single TCP packet with arbitrary sequence and acknowledgement
numbers. This will trigger \emph{stateless} censors that are only watching for
the presence of HTTP requests, regardless of surrounding connection state. In a
second experiment, we send packets that correspond to a client's side of a TCP
connection, namely the \texttt{SYN}, \texttt{ACK}, and finally \texttt{PSH+ACK}
request packets with appropriate sequence numbers. This experiment will trigger
injections from \emph{stateful} censors that expect to see evidence of a
connection before injecting.

\paragraph{TLS} Censors frequently block TLS connections based on the presence
of censored domains in the Client Hello's Server Name Indication (SNI)
extension, which indicates the domain the client is requesting in plaintext. We
craft a TLS Client Hello resembling that sent by very few other tls
implementations - we note that no widespread blocklist (or allowlist) of tls
fingerprints had been applied by censors to general TLS traffic. Similarly to
HTTP, we send both a single ``stateless'' TLS packet, and a separate
``stateful'' \texttt{SYN} / \texttt{ACK} / \texttt{PSH+ACK} sequence to trigger
censors that don't or do track TCP state respectively.

\if{0}
\paragraph{QUIC} QUIC is an encrypted UDP protocol, and like TLS, its first
packet is a Client Hello that conveys the domain in an SNI extension. Unlike
TLS, this first packet is encrypted under a secret derived from a
specification-defined value and the connection ID sent in plaintext. This allows
a network device (e.g. censor) to decrypt the QUIC Client Hello, but requires an
understanding of the QUIC specification. To test if censors are censoring QUIC
or specific domains (and decrypting Client Hello messages), we craft a standard
QUIC Client Hello message with each domain.
\fi


%We focus on identifying bidirectional censorship via injected responses to our
%probes for several protocols.  \textbf{\Cref{fig:india}} shows how bidirectional
%censorship can be measured from vantage points outside the censor state in
%question. For our study, we label any responses to probes we do not expect a
%response from as a bidirectional censorship response. In the context of this
%study, we do not expect any responses from IP addresses that are not live,
%regardless of the (protocol,domain) pair we query them for. We conduct this
%measurement for several different protocols that are known (or suspected) of
%being censored. For each experiment, we stripe IP by domain such that each
%domain is sent to all IPs before any individual IP is given a second domain.
%This is done to prevent our scanner from overwhelming any individual target
%address.
% receive responses that we can classify as either expected or characteristic of
% censorship. We do this for several different protocols that are known (or
% suspected) to be censored. For each of the following probe types we first
% conduct a measurement of control domains only to all target addresses. This is
% followed by a complete measurement in which we send probes containing all
% sensitive domains with control domains mixed in. For each experiment we stripe
% IP by domain such that each domain is sent too all IPs before any individual
% IP is given a second domain. This is done to prevent our scanner from
% overwhelming any individual target address.


\subsubsection{Controlling for responsive targets and residual censorship}
We exclude IP addresses from our scans that respond to any of our control
domains, since this indicates either a host or firewall that is likely blocking
\emph{all} requests. Typically, this is due to a host being active there,
potentially sending a \texttt{RST} packet in response to our TCP packets. Since
our goal is to measure in-network bidirectional censorship, we exclude such
``live'' hosts from our measurements.

A second related issue we consider is \emph{residual
censorship}~\cite{bock2021your}, where a censor will block a censored request,
and subsequently block future connections from the same client to the same
destination for a short time after, even if those future connections or requests
contained uncensored domains. If we interpret our results naively, residual
censorship could skew our results, making a domain appear to be censored when in
reality only a domain probed shortly before it actually was.

Thus, a limitation of our scanning methodology is that we cannot identify which
domains are blocked by a censor that employs residual censorship. Instead, we
only attempt to identify which IP addresses likely experience censorship, and
then at a country or AS level, what fraction of IPs experience censorship.
In future work, we plan to scan at a slow enough rate or to different IPs in the
same subnet to avoid the residual censorship issue.

%To account for this, we do not attempt to identify which domains a particular
%censor blocks, since this would require sending slower or having


%Due to the breadth of our measurement and the amount of IP addresses we choose as our targets, a subset of these targets are bound to be active IP addresses. Going by our definition of a bidirectional censorship response, this would introduce some false positives into our results (i.e.\ counting a response from a live IP address as a response from the bidirectional censor). Furthermore, controlling for live IP addresses is made more complicated if the censor deploys residual censorship \cite{bock2021your}. Such censors block all subsequent requests, after a sensitive query, for a certain period of time. 
%During this time, requests for control/innocuous domains would also result in a response, thereby increasing false positives.

%To cater for these scenarios, we conduct our scans in two phases. During the first phase, we send queries to all the IP addresses in our dateset but only for the control domains described in \Cref{sec:methodology:domains}. As none of these domains should trigger bidirectional censorship (as they should not be present in a censor's block-list), all responses we receive should be from live IP addresses sending back legitimate responses. For the next phase of our experiment, we repeat the experiment but with 1,400 sensitive domains instead. \textbf{We consider all responses from IP addresses that responded only during the second phase as bidirectional censorship responses.}



\if0
\subsubsection{TCP}
While some firewall implementations explicitly look for singular TCP (TLS, HTTP,
or other) packets that violate their rules, others keep a modest amount of state
and require the TCP flow to be ``established'' before they will present
censorship behavior. However, because routing on the internet is not typically
symmetrical and response traffic often follows alternative routes, some high
performance firewall implementations will trigger censorship behavior with just
the unidirectional client-to-station flow of a TCP SYN packet, followed by an
ACK packet and a data packet with the PSH/ACK flags set. While this allows flows
that use heterogeneous routing to be included and censored, it also allows
falsified flows with non-existent endpoints to trigger a censorship response as
there is no validation that the TCP handshake successfully completed.

We take advantage of this by measuring censorship responses triggered by a
singular TCP PSH/ACK packet with data, as well as responses triggered by a
packet sequences of SYN -- ACK -- PSH/ACK with data.

Our \textbf{HTTP} probe data consists of a HTTP request crafted to trigger
censorship using blocklisted domains in the \texttt{HOST} header.
% Add/ Test Keyword based blocking?

Our \textbf{TLS} probe data consists of a TLS \texttt{ClientHello} crafted to
trigger censorship using blocklisted domains in the SNI extension.

\subsubsection{UDP}
To measure \textbf{DNS} we send one {\tt A} and one {\tt AAAA} query per domain
to each of our selected target addresses. This allows us to make a loose, but
direct comparison of censorship rates for IPv4 and IPv6 resource records.


Our \textbf{Quic} probe consists of a client initial frame that contains a
TLS1.3 {\tt ClientHello} with the domain in question in the SNI extension. In
following with the Quic specification the payload portion of the frame is
encrypted using an a key derived using an HKDF on our selected connection ID.
While on-path attackers can decrypt this initial packet and inspect the
plaintext TLS packet including SNI, they are forced to perform a relatively
costly HKDF in order to do so.
\fi

\subsubsection{Tagging}

Similar to the architecture of the ZMap~\cite{Durumeric13zmap} scanning tool,
our probing architecture uses many threads to craft and send packets and one
independent thread to listen and ingests responses. One consequence of this
architecture is that we must maintain a limited amount of state internally for
each connection we create. While injected responses to DNS probes may include
the host name (in the response Resource Record) other protocols are not
guaranteed to do so. For example, the TCP RST packets injected by the GFW in
response to a TLS probe with a censored SNI will not indicate what domain the
original probe included. Similar challenges exist for HTTP probes.

To solve this problem we ``tag'' outgoing packets in a way that injected
responses will echo this tag, and allow us to identify the details of the probe
they correlate to, as well as check the
validity of the response without tracking the full connection state from start
to finish.

We start by creating a 1-to-1 mapping from domain to a random number in the
range 1000-65535. We use this number as the source port for the outgoing packet
meaning that we can use the destination port of any response packet to lookup
the domain sent in the original probe. In order to ensure that response TCP
packets are associated with our measurement and not just sent randomly we set
the acknowledgement number of the outgoing probe to be the CRC32 of the source
port (from our domain mapping) and the target address. This allows our ingest
thread to quickly validate responses by checking:
\begin{gather*}
CRC32(PORT_{dst},ADDR_{src}) \stackrel{?}{=} SEQ - Len
\end{gather*}

\if{0}
For Quic responses that either return garbage or change the connection ID for
the server initial packet we need to have access to the 8 byte connection ID
sent in our probe. To make sure this is always available we set the source port
for the outgoing packets from our 1-to-1 domain map and then set the connection
ID to be the CRC64-ECMA of the source port and the target address for the
outgoing packet. That way response packets can statelessly derive the original
connection ID by computing the following for incoming packets.
\begin{gather*}
Conn\_ID = CRC64_{ECMA}(PORT_{dst},ADDR_{src})
\end{gather*}
\fi

\subsection{Ethics}\label{sec:methodology:ethics}

Our experimental design has incorporated ethical considerations into the
decision-making process at multiple stages. Censorship measurement has inherent
risks and trade-offs: better understanding of censorship can help support and
inform users, but specific measurements may carry risk to participants or
network users. Measurement of bidirectional censorship typically allows
researchers to limit the number of third parties implicated in experiments as
the censorship response can be triggered by either ingress or egress traffic
removing the need for cooperation by individual hosts or hosting services within
a censoring region. Vantage points are instead hosted in regions that do not
censor connections and allow for researchers to freely measure the internet.

The vantage that was used for data collection is connected to the internet with
a 1 Gbps interface that scanned using the default rates for our custom protocol
scanning tool (line rate). However, the structure of the scan was established
such that individual addresses and domains would be accessed in round robin
order --- \ie when sending probes every target address would receive a first
request before any target would receive the subsequent request.

We encourage readers to consult The Menlo Report~\cite{menlo}, its companion
guide~\cite{menlo-companion}, and the censorship specific ethical measurement
guidelines outlined by Jones \etal \cite{jones2015ethical} for further
discussion of ethical design for internet measurement.
